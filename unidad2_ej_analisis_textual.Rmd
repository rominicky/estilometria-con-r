---
title: "ejercicio-analisis-textual"
author: "Romina De León"
output: 
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

```{css, echo=FALSE}
body {
  padding-bottom: 52px;
}
```

# Comenzando con análisis textual en R Studio

#### **Romina De León**

[rdeleon\@conicet.gov.ar](rdeleon@conicet.gov.ar)

Durante esta ejercitación utilizarás lo visto en las unidades previas.

El corpus que he dejado en la carpeta del campus, está compuesto por
discursos del presidente A. Fernández y Decretos de Necesidad de
Urgencia (DNU) dictados por el Poder Ejecutivo de la Argentina. Estos se
presentaron a la población del país desde el inicio del Distanciamiento
Social, Preventivo y Obligatorio (DIASPO) el 13 de marzo, hasta el 30 de
junio de 2020[^1].

[^1]: Accesibles desde <http://www.infoleg.gob.ar/?page_id=112> y
    <https://www.casarosada.gob.ar/informacion/discursos?start=200>.

El corpus fue denominado según el siguiente orden: `mes-día-dnu o disc`
en formato de texto plano (`txt`). En la carpeta zippeada encontrarás,
`disc-dnu`, los archivos listado y una segunda carpeta que se llama
`discurso` que es donde me ubicaré para realizar este ejercicio.

Ello significa que solo se utilizarán los primeros cuatro archivos,
`0313disc.txt`;`0316dnu.txt`; `0318dnu.txt` y `0320disc.txt`. Esto fue
pensado así, para poder comenzar con pequeñas visualizaciones de los
archivos, donde se podrás observar comparaciones de días, para luego
pasar a generalizar con los 47 textos del corpus.

```{r eval=FALSE, error=FALSE, warning=FALSE}
rm(list = ls())

getwd()

```

**Primero paso**: Puedes descargar y descomprimir como lo realizas
habitualmente la carpeta directamente del campus, o sino mediante el
comando `download.file` descargarás el archivo \*.zip con la url entre
comillas, luego indicarás el nombre del archivo y finalmente con el
parámetro **wget** se indica el método utilizado para realizar la
descarga, **aclaración** en Windows debes cambiarlo por **wininet**.
Luego, descomprime el archivo con el comando `unzip`. A continuación
ingresa a tu carpeta de trabajo.

```{r eval=FALSE, error=FALSE, warning=FALSE}

download.file("https://github.com/hdcaicyt/Estilometria-con-R/blob/4678a1e2454287f853c14ae494422a96e38af05f/disc-dnu.zip", destfile="disc-dnu.zip", "wget") # Aclaración para Windows reemplazar 'wget' por "wininet"

unzip("disc-dnu.zip")

setwd('/disc-dnu/discurso/') # Ingresa a la carpeta que se ha creado, recuerda que puedes incluso crear otra carpeta para datos de entrada (input) y datos de salida (output) para cargar y guardar tus archivos
```

El paso a seguir es llamar a las librerías que se utilizarán:

-   <code>tidyverse</code> y <code>tidytext</code> fueron explicados en
    la [Unidad
    1](https://hdlab.space/Estilometria-con-R/unidad1_manipulacion-de-datos.html).
-   <code>ggplot2</code> es un paquete que permite la visualización de
    datos, implementa una representación organizada y en capas, de
    marcos, ejes, textos, títulos, etc., utilizando diferentes colores,
    símbolos, tamaños, etc. Fue desarrollada por L. Wilkinson et. al en
    2000[^2]. No hace falta llamarlo pues se encuentra incluido dentro
    de **tidyverse**, sin embargo, es fundamental considerarlo porque es
    muy útil en R.

[^2]: Para más información sobre ggplot2, véase Wilkinson, L., D. Wills,
    D. Rope, A. Norton, and R. Dubbs (2005. The Grammar of Graphics.
    Statistics and Computing. New York: Springer.

```{r eval=TRUE, message=FALSE, warning=FALSE}

library("tidytext", "tidyverse")

```

Una vez instalados estos requerimientos, llamarás a la función
<code>list.file</code> para corroborar que los archivos necesarios se
encuentran en la carpeta donde estás trabajando.

```{r eval=FALSE, message=FALSE}

list.files('discurso')
```

En un segundo paso, conformarás una nueva **lista**, a la que puedes
llamar `archivos`, cuyos elementos serán los textos planos dentro de la
carpeta **discurso**, para ello, utilizarás, nuevamente, la función
`list.files`, y le indicarás que tipo de archivo debe contener, **txt**,
con la declaración `pattern`. Recuerda estar trabajando en la carpeta
donde se encuentran tus archivos, sino deberás incorporar un declaración
que indique la ubicación, `path =`, por ejemplo:
`list.files(path = "discurso", pattern = "\\.txt$")`.

```{r eval=TRUE}

archivos <- list.files(pattern = "\\.txt$")

archivos #corroborá que la lista está correcta

class(archivos) #la función 'class()' devuelve el tipo de objeto, en este caso podrás ver que se trata de una lista de caracteres

length(archivos) #la función 'length()' permite obtener el tamaño del objeto que se incluye dentro de los paréntesis

```

Tercer paso, será para facilitar la visualización de cada texto en los
siguientes pasos, por lo cual, se eliminará lo que no es necesario del
nombre de cada archivo. Para ello, utilizarás la función `gsub`, de gran
utilidad para limpieza de datos que está incluida por defecto en **R**.
Los parámetros indicarán, según orden de escritura, caracteres que se
eliminarán (\\\\. será para indicar que se busca un **`.`** y no otro
carácter según las reglas de expresión regular), los que se agregarán
(las doble comillas indican que no se agregará nada), y finalmente, con
**perl**, se determina que se utilicen las reglas de expresiones
regulares[^3].

[^3]: Puedes acceder a más información de RegEx accediendo a
    <https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular>.

```{r eval=TRUE, message=TRUE, warning=TRUE}
textos_archivo <- gsub("\\.txt", "", archivos, perl = TRUE)

#Ahora llama a la lista para corroborar que se ha realizado la limpieza

textos_archivo

```

Cuarto paso, con la lista de textos acorde para continuar el ejercicio,
procede a armar un `tibble`, este objeto es muy similar a los data.frame
presentados en la unidad anterior, pues es rectangular, organizado en
filas y columnas, y pertenece al paquete `tidyverse`. Existen tres
diferencias entre los **data.frame** y los **tibble**, la primera es
como se muestran en consola; segunda, los tibble eliminan por defecto
**rownames**, e incluso no se recomienda su uso, para evitar problemas
de compatibilidad principalmente con bases de datos SQL; por último, no
convierte a las **string** (cadenas de caracteres) en factores.
Igualmente, ambos objetos son intercambiables[^4].

[^4]: Puedes acceder a más información sobre **tibble** en
    <https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html>.

Armarás el tibble con tres columnas, la primera y última tendrán
caracteres dentro de sus variables, y `parrafo` será numérica, como se
muestra a continuación:

```{r eval=TRUE, error=FALSE, warning=FALSE}

mensajes <- tibble(textos_archivo = character(),
                   parrafo = numeric(),
                   texto = character())

#Si llamas a "mensajes" comprobarás que figura como vacío, esto es porque aún no se ha completado con datos, ello lo realizarás en el próximo paso.

mensajes

```

Para poder completar el tibble **mensajes** que has creado, realizarás
una iteración con el bucle `for`, que permite repetir instrucciones,
evaluando el mismo código para cada elemento de un vector o lista.

En este ejercicio, se le indica que comience la iteración en 1 hasta el
tamaño (**length**) de `archivos`. Luego, armará en el objeto `discurso`
lo que leerá con la función `read_lines`, allí se indicará que con la
función `paste` junte los textos dentro de tu carpeta de trabajo,
***importante añadir esa dirección***, con el contenido de `archivos[i]`
, cuyo valor cambiará en cada repetición del bucle. El parámetro `sep =`
será para indicar que los pegue con "**/"** .

Paso siguiente, convertir lo que acaba de leer en una nueva tabla, otra
tibble llamada `temporal`, tal como hiciste con la anterior de
`mensajes`, indicando que complete con cada texto. La primer columna,
`parrafo` indicará el número de párrafos de cada texto, se lo adjudicará
con la función `seq_along` que contará hasta el número máximo en el
objeto `discurso` y por último, en `texto` se agregará cada párrafo.
Para finalizar la repetición, indicarás que en **mensajes** se sume la
tabla `temporal`, mediante la función **bind_rows**.

```{r eval=TRUE}

#Iteración 

for (i in 1:length(archivos)){
  discurso <- read_lines(paste("C:/hdcaicyt/R/ejercicio/discurso",
                               archivos[i],
                               sep = "/"))
  temporal <- tibble(textos_archivo = textos_archivo[i],
                     parrafo = seq_along(discurso),
                     texto = discurso)
  mensajes <- bind_rows(mensajes, temporal)
} 

mensajes #corrobora que se creó correctamente la tabla
```

Este sexto paso, será para proceder con la tokenización del corpus, es
decir extraer sus unidades de análisis. Para ello, se utilizará la
función `unnest_tokens` que se encuentra dentro del paquete
**tidytext**; se indicará que a cada fila de **parrafo**, lo separe a
nivel de palabra según la columna **texto**.

**mensajes_palabras** estará formado por tres columnas, una que indicará
el texto, la otra el número de párrafo y finalmente, las palabras de
cada uno, pero separadas. Recuerda que el operador **pipe (%\>%)** fue
presentado en la primera unidad.

```{r eval=TRUE}

mensajes_palabras <- mensajes %>% 
  unnest_tokens(palabra, texto)


mensajes_palabras

```

Como siguiente paso, se eliminarán las palabras vacías, que no serán
útil en este ejercicio, pero si pueden servir para otros análisis;
asimismo, lo realizarás para continuar con la parte de ejercitación que
solicitaré como extra. Para ello, se armará una una tabla con las
palabras que se obtienen de la función **get_stopwords** que forma parte
del paquete **tidytext**, y se aclarará que se utilice en español.

```{r eval=TRUE}

stopwords1 <- get_stopwords("es")

stopwords1
```

Paso siguiente, se le indicará que cambie la columna **word** por
**palabra**, mediante la función `rename`, que forma parte del paquete
**dplyr**, que a su vez pertenece a **tidyverse**, es útil para cambiar
el nombre de variables u objetos. Cuestión que debe considerarse al
momento de renombrar, es que primero debe escribirse el nuevo término, y
a continuación el que se va a cambiar. Este paso es importante para
poder eliminar las palabras vacías de mensajes_palabras.

```{r eval=TRUE}

stopwords1 <- stopwords1 %>%
rename(palabra = word)

stopwords1
```

Ahora, para poder obtener una nueva lista con palabras vacías excluidas,
se usará la función `anti_join` que compara las columnas con el mismo
nombre, en este caso **palabra** y elimina los elementos que coinciden.
Cuando finalice la ejecución, aparece un mensaje que significa que las
dos listas tienen iguales elementos en esas columnas, en caso contraria,
devolverá mensaje de error.

```{r eval=TRUE}

mensajes_sin_vacias <- mensajes_palabras %>%
anti_join(stopwords1)
```

Como paso siguiente, se procederá a la limpieza de la lista
**mensajes_palabras** y **mensajes_sin_vacias**. Para ello, se
eliminarán dígitos, palabras con menos de tres caracteres, celdas sin
caracteres y las que contengan puntos, todo ello se realizará en la
columna **palabra**. Por lo cual, se utilizarán las siguientes
funciones:

-   **mutate**, que crea, modifica y elimina columnas según ciertos
    parámetros que se le adjudican, se encuentra dentro del paquete
    **tidyverse**. En este caso se le sumará **str_remove_all** (función
    que se utiliza para eliminar coincidencias en una cadena de
    caracteres) para que elimine en la columna **palabra** los números
    que aparezcan.

-   **filter**, será para filtrar las palabras con menos de tres
    caracteres, con la ayuda de **nchar** que cuenta el número de
    caracteres; también se utilizará para eliminar los espacios con
    valores de caracteres vacíos y cuando encuentre un **.** o **n°**
    (símbolo de número que figura en los textos).

Repetirás las limpieza en las dos listas, recuerda guardar las que no
fueron limpias, adjudicando nuevos nombres a las que si se hayan
limpiado.

```{r echo=TRUE}


mensajes_pal_limpio <- mensajes_palabras %>%
mutate(palabra = str_remove_all(palabra, "\\d+")) %>%
filter(nchar(palabra) > 3) %>% 
filter(palabra != "") %>%   
filter(palabra != ".") %>%   
filter(palabra != "n°")


mensajes_pal_limpio


```

Indica nuevo nombre a esta tabla que se ha limpiado. Esta nueva tabla
será útil para realizar la parte de ejercitación extra.

```{r echo=TRUE}

mensajes_limpio <- mensajes_sin_vacias %>%
mutate(palabra = str_remove_all(palabra, "\\d+")) %>%
filter(nchar(palabra) > 3) %>% 
filter(palabra != "") %>%   
filter(palabra != ".") %>%   
filter(palabra != "n°")


mensajes_limpio

```

En el paso que sigue, se dará inicio a los análisis de frecuencia y
gráficos de los textos del corpus. Para lo cual, primero se contará las
palabras que se encuentran dentro de la columna **palabra** en
**mensajes_pal_limpio**, por medio de la función **count**, que también
pertenece al paquete **tidyverse**, y presentará la frecuencia absoluta
de cada una, o sea la cantidad de veces que se repite.

```{r eval=TRUE}

mensajes_pal_limpio %>% 
  count(palabra, sort = TRUE)

```

Como la frecuencia absoluta no representa ningún dato novedoso, se
calculará el promedio de palabras por texto, que no han sido procesadas
con la limpieza de datos, pudiéndose calcular entre el resultado del
número de columnas obtenidos con la función **nrow** y el tamaño de la
lista archivos, calculado en los primeros pasos.

```{r eval=TRUE}

nrow(mensajes_palabras) / length(archivos)


```

Ese dato solo indicará un promedio, por lo cual, a continuación se
calculará la frecuencia relativa por palabras y será adjudica como una
nueva columna, dentro del tibble limpio: **mensajes_pal_limpio,** que se
denominará **relativa**. Por lo cual tendremos tres columnas, la del
término, su frecuencia absoluta y la relativa. Por ello armaremos un
nuevo tibble denominado, **mensajes_frecuencias**:

```{r eval=TRUE}

mensajes_frec_limpio <- mensajes_pal_limpio %>%
  count (palabra, sort = TRUE) %>%
  mutate (relativa = n / sum(n))

mensajes_frec_limpio
```

También, podrías pedir que devuelva la frecuencia por cada texto, para
ello, se deberá agrupar por cada uno mediante la función **group_by**,
del paquete **tidyverse**, únicamente indicando que debe considerar la
lista de textos, antes preparada; luego del cálculo de la frecuencia
absoluta, debe crearse una nueva columna para la frecuencia relativa, y
finalmente que desagrupe los datos, con **ungroup()**. Todo ello, será
guardado en otra tabla **frecuencia_text_arch**:

```{r}


frecuencia_text_arch <- mensajes_pal_limpio %>%
           group_by(textos_archivo) %>%
           count(palabra, sort = T) %>%
           mutate(relative = n / sum(n)) %>%
           ungroup()

frecuencia_text_arch
```

Ahora, para visualizar estos datos calculados realizarás un gráfico,
donde se presentará la frecuencia por cada texto, con la función
**ggplot**, que fue convocada al principio de la práctica con el paquete
**tidyverse**, se le indicará que se graficará en barras, mediante el
parámetro **geom_bar**, luego se indicará con **aes**, el contenido de
los eje **X** e **Y**, en ese orden; y por último, con **stat**, que
tipo de transformación estadísticas tendrán los datos.

```{r eval=TRUE, message=FALSE}
  
mensajes_pal_limpio %>%
           group_by(textos_archivo) %>%
          count(palabra, sort = T) %>%
           ggplot() +
           geom_bar(aes(textos_archivo,
                        n),
                    stat = 'identity')

ggsave("mensajes_pal_limpio.png")
```

Otro gráfico que se puede realizar es observando las palabras más
frecuentes de los textos, para ello primero, se llamará a
**mensajes_pal_limpio**, se pedirá que cuente las ocurrencias, ordenadas
de mayor a menor; en la tercera línea se indicará que filtre las
ocurrencias mayores a 10, pues se han eliminado las palabras vacías.
Como se explicó antes, al crear una nueva tabla interna se debe indicar
con **mutate**, la nueva columna, que almacenará los nuevos datos. Luego
se indicarán los parámetros del gráfico. *Aclaración, **ggplot** une sus
parámetros con **+***; el argumento **fill** hará que cada palabra tenga
un color determinado, **stat** indicará que las barras tendrán la altura
según **n**. Las líneas siguientes presentarán parámetros para título,
ubicación de leyenda y etiquetas en los ejes; y con **coord_flip()** se
asegura que el eje **Y** se imprima de forma horizontal.

```{r eval=TRUE}

mensajes_pal_limpio %>%
count(palabra, sort = T) %>%
filter(n > 10) %>%
mutate(palabra = reorder(palabra, n)) %>%
ggplot(aes(x = palabra, y = n, fill = palabra)) +
geom_bar(stat="identity") +
theme_minimal() +
theme(legend.position = "none") +
ylab("Número de veces que aparecen") +
xlab(NULL) +
ggtitle("Discurso-DNU") +
coord_flip()

ggsave("mensajes_palabras.png")
```

Otra utilidad, que ha sido explicada en la parte teórica de la **Unidad
2**, es la Ley de Zipf, por lo cual, se reorganizarán los parámetros de
ggplot para poder apreciar la relación entre cada palabra y su
frecuencia.

Para ello, primero se armará una tabla con **mensajes_palabras**, con
columnas con frecuencia absoluta = **n** y frecuencia relativa, sumada
una última columna, que indicará el orden de palabras en forma
descendente, o el número de filas, llamada **Clasificacion**.

```{r}

mensajes_frecuencias <- mensajes_palabras %>%
  count (palabra, sort = TRUE) %>%
  mutate (relativa = n / sum(n)) %>% 
   mutate(Clasificacion = row_number())

mensajes_frecuencias
  
```

Para realizar el gráfico con dichos datos, se utilizará la tabla
anterior, y se indicará en la tercera línea, mediante **geom_line** la
forma en que va a realizar la línea del gráfico; y en las últimas dos,
se indicará que en ambos ejes se usará la escala logarítimica.

```{r eval=TRUE}

mensajes_frecuencias %>%  
  ggplot(aes(Clasificacion, relativa, color = n)) + 
  geom_line(size = 1.5, alpha = 1.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```

Ahora, te propongo que realices los análisis y gráficos anteriores para
los mensajes donde se han realizado la limpieza de palabras vacías y
compares los resultados. Como ejercitación extra podrías, además,
intentar observar los cambios al considerar todos los archivos del
corpus.
