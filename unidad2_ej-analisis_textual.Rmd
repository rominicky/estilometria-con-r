---
title: "ej-analisis_textual"
author: "Romina De León"
date: "21/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
body {
  padding-bottom: 52px;
}
```

## Comenzando con análisis textual en R Studio

Durante esta ejercitación utilizaremos todo lo visto en las unidades previas. 
En primer lugar, nos ubicaremos dentro de nuestra carpeta, donde se encuentran los archivos de nuestro corpus. Vale aclarar, que les he dejado más de 40 textos, que son discursos y Decretos de Necesidad de Urgencia, que se realizaron desde el inicio del Distanciamiento Social, Preventivo y Obligatorio el 13 de marzo, hasta el 30 de junio de 2020, han sido nombrados:mes-dia-dnu o disc. Asimismo, este ejercicio, lo he realizado con los primeros cuatro archivos, 0313disc.txt; 0316dnu.txt; 0318dnu.txt y 0320disc.txt 

```{r eval=TRUE}
getwd()

#Deben modificar su ubicación

setwd("C:/hdcaicyt/R/discurso/prueba1")

```


Luego instalaremos y llamaremos a las librerías que utilizaremos

```{r eval=TRUE}


suppressPackageStartupMessages({

 library(stopwords)
library(tidytext)
library(tidyverse)
library(tm)
library(stringr)
library(dplyr)
library(ggplot2)
  
})

```


Ahora chequearemos que están en dicha carpeta los archivos con los que trabajaremos 

```{r eval=FALSE}

list.files('C:/hdcaicyt/R/discurso/prueba1/')
```


A continuación, haremos una nueva lista con los elementos de texto plano que se encuentren en la carpeta, para ello, utilizaremos la función <code>list.files</code> y aclararemos que tipo de archivo, y que debe contener.

```{r eval=TRUE}

archivos <- list.files("./", pattern = "\\.txt$", full.names = TRUE)

# Podemos llamar a la lista creada, considerar que tipo de elemento y su tamaño.

archivos
class(archivos)
length(archivos)

```


Posteriormente para facilitar nuestro trabajo, eliminaremos lo que no es necesario del nombre

```{r eval=TRUE, message=TRUE, warning=TRUE}
textos_archivo <- gsub("\\.txt", "", archivos, perl = TRUE)

#La función gsub es una función para la limpieza de datos, con ello eliminaremos la extensión de cada elemento dentro de la lista.

textos_archivo


```

```{r echo=TRUE, message=FALSE, warning=FALSE}


#También eliminaré './' que quedaron dentro de mi lista

textos_archivo <- gsub("./", "", textos_archivo, perl = TRUE)

textos_archivo

```


Para poder continuar con el análisis de nuestro corpus armaremos un tibble, que es otro tipo de lista, como el dataframe. 

```{r eval=TRUE}

mensajes <- tibble(textos_archivo = character(),
                   parrafo = numeric(),
                   texto = character())

#Luego de tibble, hemos indicado que será cada columna y que tiene de elementos contendrá

mensajes

#Nos figura que nuestra tibble se encuentra vacía, por ello a continuación la completaremos con nuestros datos.

```

Para secuenciar el corpus, realizaremos una iteración con la función <code>for</code>, en la que indicaremos que tome cada uno de los elementos de <code>archivos</code>, lea cada elemento, los separe, y complete el tibble que creamos en el paso anterior.

```{r eval=TRUE}

#secuenciamos nuestro corpus 

for (i in 1:length(archivos)){
  discurso <- read_lines(paste("C:/hdcaicyt/R/discurso/prueba1/",
                               archivos[i],
                               sep = "/"))
  temporal <- tibble(textos_archivo = textos_archivo[i],
                     parrafo = seq_along(discurso),
                     texto = discurso)
  mensajes <- bind_rows(mensajes, temporal)
} 

temporal
mensajes


```

Una vez que hemos completado el tibble, procederemos con la tokenización de nuestro corpus, es decir extraeremos sus unidades de análisis.

```{r eval=TRUE}

mensajes_palabras <- mensajes %>% 
  unnest_tokens(palabra, texto)


mensajes_palabras

```


Como podemos observar se encuentran palabras que no son necesarias y hacen que nuestro corpus extenso, para ello eliminaremos las stopwords.


```{r eval=TRUE}

#primero instalaremos el paquete de stopwords, eliminamos las palabras vacías, con la función filter y agregando que estamos trabajando en español.

stopwords1 <- get_stopwords("es")

stopwords1

#Nos ha armado otro tibble con las palabras vacías

#Para que pueda limpiar nuestra tibble, debemos renombrar, la columna word por "palabra"

stopwords1 <- stopwords1 %>%
rename(palabra = word)

stopwords1

```

Importante, para cambiar nombres primero debes poner el nuevo término y a continuación el que deseas cambiar.

```{r eval=TRUE}

mensajes_limpio <- mensajes_palabras %>%
anti_join(stopwords1)

#El mensaje que nos aparece, es porque ambas tibble tiene la misma variable común "palabra", si no la encontrase devolvería error.
```


```{r echo=TRUE}
#Aquí eliminaremos los dígitos, pues no influyen en nuestro análisis


mensajes_limpio <- mensajes_limpio %>%
mutate(palabra = str_remove(palabra, "\\d+"))

# NIDIA
# acá traté de limpiar los NA que me quedaban luego de eliminar los números pero o me salta error o no cambia nada! :( 

mensajes_limpio

```

Una vez que hemos limpiado y tokenizado nuestro corpus, procederemos a realizar análisis de frecuencia y gráficos


```{r eval=TRUE}

#Con estas líneas podremos observar palabras y sus frecuencias absolutas

mensajes_palabras %>% 
  count(palabra, sort = TRUE)

mensajes_limpio %>% 
  count(palabra, sort = TRUE)


```



```{r eval=TRUE}

#Ahora comenzaremos con los análisis del corpus que no fue limpiado 

nrow(mensajes_palabras) / length(archivos)

#En la línea anterior hemos calculado el promedio de palabras por discurso o dnu.

#Con la próxima instrucción calcularemos la frecuencia relativa por palabras y adjudicaremos ese nuevo valor en una columna que se denominará 'relativa'. Por lo cual tendremos tres columnas, la del término, su frecuencia absoluta y la relativa.

mensajes_palabras %>% count (palabra, sort = TRUE) %>% mutate (relativa = n / sum(n))

 # Para poder almacenar estos datos crearemos una nueva tabla denominada 'mensajes_frecuencias'.

mensajes_frecuencias <- mensajes_palabras %>%
           count(palabra, sort = TRUE) %>%
           mutate(relative = n / sum(n))    

#Si quisieramos que nos devuelva la frecuencia por cada discurso o dnu, debemos agrupar por cada uno de ello, lo realizamos con la función 'group_by' y lo guardamos en otra tabla llamada 'frecuencia_text_arch'

frecuencia_text_arch <- mensajes_palabras %>%
           group_by(textos_archivo) %>%
           count(palabra, sort = T) %>%
           mutate(relative = n / sum(n)) %>%
           ungroup()

#Para finalizar podemos graficar todas estas mediciones         
         
mensajes_palabras %>%
           group_by(textos_archivo) %>%
          count(palabra, sort = T) %>%
           ggplot() +
           geom_bar(aes(textos_archivo,
                        n),
                    stat = 'identity')

```



```{r eval=TRUE}

# Ahora graficaremos las palabras más frecuentes de discursos y dnu, para ello primero llamaremos a nuestra tabla 'mensajes_palabras' luego le diremos que cuente las ocurrencias, ordenadas de mayor a menor, en la tercera línea indicamos que filtre las ocurrencias mayores a 15. Como explicamos antes, como se crea una nueva tabla interna debemos indicar con mutate, la nueva columna, que almacenará los nuevos datos. Luego daremos las indicaciones para graficar con la función 'ggplot'. Primera aclaración, esta función une sus líneas con '+'; el argumento fill hará que cada palabra tenga un color determinado, 'stat' indica que las barras tomaran la altura según 'n'. Las líneas siguientes indicaran título, donde se ubicará la leyenda, y etiquetas en los ejes; y con 'coord_flip()' nos aseguramos que el eje 'Y' se imprima de forma horizontal. 

mensajes_palabras %>%
count(palabra, sort = T) %>%
filter(n > 15) %>%
mutate(palabra = reorder(palabra, n)) %>%
ggplot(aes(x = palabra, y = n, fill = palabra)) +
geom_bar(stat="identity") +
theme_minimal() +
theme(legend.position = "none") +
ylab("Número de veces que aparecen") +
xlab(NULL) +
ggtitle("Discurso-DNU") +
coord_flip()
```

         
Ahora, te propongo que realices los análisis y gráficos anteriores para <code>mensajes_limpio</code> y compares los resultados.
