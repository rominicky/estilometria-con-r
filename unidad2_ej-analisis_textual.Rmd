---
title: "ej-analisis_textual"
author: "Romina De León"
date: "21/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
body {
  padding-bottom: 52px;
}
```

## Comenzando con análisis textual en R Studio

Durante esta ejercitación utilizaremos todo lo visto en las unidades previas. 
En primer lugar, nos ubicaremos dentro de nuestra carpeta, donde se encuentran los archivos de nuestro corpus. Vale aclarar, que les he dejado más de 40 textos, que son discursos y Decretos de Necesidad de Urgencia, que se realizaron desde el inicio del Distanciamiento Social, Preventivo y Obligatorio el 13 de marzo, hasta el 30 de junio de 2020, han sido nombrados:mes-dia-dnu o disc. Asimismo, este ejercicio, lo he realizado con los primeros cuatro archivos, 0313disc.txt; 0316dnu.txt; 0318dnu.txt y 0320disc.txt 

### NH: Acá habría que explicar de dónde salieron los textos (url de la fuente). También explicar porqué se usan sólo 4 archivos si el corpus es mayor


```{r eval=TRUE}
getwd()

#Deben modificar su ubicación

setwd("C:/hdcaicyt/R/discurso/prueba1")

```


Luego instalaremos y llamaremos a las librerías que utilizaremos
### NH: no está de más mostrar cuáles son las que tienen que instalar con "install.packages()". 

```{r eval=TRUE}


suppressPackageStartupMessages({ ## NH: esto es para que no imprima los mensajes de carga de paquetes en la notebook? Me parece mejor poner {r message=FALSE} al inicio de la celda. Esto evita los mensajes de instalación sin que aparezca nada raro en el html final

 library(stopwords)
library(tidytext)
library(tidyverse)
library(tm)
library(stringr) ### NH: no es necesario llamarla aparte, es parte del tidyverse
library(dplyr) ### NH: no es necesario llamarla aparte, es parte del tidyverse
library(ggplot2)
  
})

```


Ahora chequearemos que están en dicha carpeta los archivos con los que trabajaremos 

```{r eval=FALSE}

list.files('C:/hdcaicyt/R/discurso/prueba1/')
#list.files('corpus-dnu-disc') ### NH: Es mejor que el path del ejercicio coincida con el nombre de la carpeta que dejamos en el campus, si no, va a haber muchos problemas
```


A continuación, haremos una nueva lista con los elementos de texto plano que se encuentren en la carpeta, para ello, utilizaremos la función <code>list.files</code> y aclararemos que tipo de archivo, y que debe contener.

```{r}
archivos <- list.files("C:/hdcaicyt/R/discurso/prueba1/", pattern = "\\.txt$", full.names = TRUE)
#archivos <- list.files("corpus-dnu-disc", pattern = "\\.txt$", full.names = TRUE) ### NH:

# Podemos llamar a la lista creada, considerar que tipo de elemento y su tamaño.

archivos
class(archivos) 
length(archivos) ### NH: explicar para qué se usa lenght

```


Posteriormente para facilitar nuestro trabajo, eliminaremos lo que no es necesario del nombre

```{r eval=TRUE, message=TRUE, warning=TRUE}
textos_archivo <- gsub(".txt", "", archivos, perl = TRUE)
#textos_archivo <- gsub("corpus-dnu-disc/", "", textos_archivo, perl = TRUE) ### NH: para que funcione en el nuevo path

#La función gsub es una función para la limpieza de datos, con ello eliminaremos la extensión de cada elemento dentro de la lista.

textos_archivo


```

```{r echo=TRUE, message=FALSE, warning=FALSE}


#También eliminaré './' que quedaron dentro de mi lista
### NH: No entiendo este paso

textos_archivo <- gsub("./", "", textos_archivo, perl = TRUE)

textos_archivo

```


Para poder continuar con el análisis de nuestro corpus armaremos un tibble, que es otro tipo de lista, como el dataframe. 

```{r eval=TRUE}

mensajes <- tibble(textos_archivo = character(),
                   parrafo = numeric(),
                   texto = character())

#Luego de tibble, hemos indicado que será cada columna y que tiene de elementos contendrá

mensajes

#Nos figura que nuestra tibble se encuentra vacía, por ello a continuación la completaremos con nuestros datos.

```

Para secuenciar el corpus, realizaremos una iteración con la función <code>for</code>, en la que indicaremos que tome cada uno de los elementos de <code>archivos</code>, lea cada elemento, los separe, y complete el tibble que creamos en el paso anterior.

```{r eval=TRUE}
### NH: acá estaría mejor tener un output interactivo, poder navegar por la tibble

#secuenciamos nuestro corpus 

for (i in 1:length(archivos)){
  # discurso <- read_lines(paste("C:/hdcaicyt/R/discurso/prueba1/",
  #                              archivos[i],
  #                              sep = "/"))
  discurso <- read_lines(archivos[i]) ### NH: para que funcione en el nuevo path
  temporal <- tibble(textos_archivo = textos_archivo[i], ### NH: por qué se crea esta variable temporal?
                     parrafo = seq_along(discurso),
                     texto = discurso)
  mensajes <- bind_rows(mensajes, temporal)
} 

temporal
mensajes


```

Una vez que hemos completado el tibble, procederemos con la tokenización de nuestro corpus, es decir extraeremos sus unidades de análisis.

```{r eval=TRUE}

mensajes_palabras <- mensajes %>% 
  unnest_tokens(palabra, texto)

mensajes_palabras

```


Como podemos observar, se encuentran palabras que no son necesarias y hacen que nuestro corpus extenso, para ello eliminaremos las stopwords. ### NH: ojo, depende para qué. Las stopwords son muy relevantes para clustering automático, relativizar esta afirmación.


```{r eval=TRUE}

#primero instalaremos el paquete de stopwords, eliminamos las palabras vacías, con la función filter y agregando que estamos trabajando en español.
### NH: dónde está el código de instalación del paquete de stopwords? en dónde se usa filter? Si no lo tienen instalado les va a dar error esta parte.

stopwords1 <- get_stopwords("es")

stopwords1

#Nos ha armado otro tibble con las palabras vacías

#Para que pueda limpiar nuestra tibble, debemos renombrar, la columna word por "palabra"

stopwords1 <- stopwords1 %>%
rename(palabra = word)

stopwords1

```

Importante, para cambiar nombres primero debes poner el nuevo término y a continuación el que deseas cambiar. ### NH: cambiar nombres de qué? Acá no estamos modificando una variable sino creando una nueva

```{r eval=TRUE}

mensajes_limpio <- 
  mensajes_palabras %>%
  anti_join(stopwords1)

#El mensaje que nos aparece es porque ambas tibble tienen variable común "palabra", si no la encontrase devolvería error.
```


```{r echo=TRUE}
#Aquí eliminaremos los dígitos, pues no influyen en nuestro análisis

mensajes_limpio <- 
  mensajes_limpio %>%
  mutate(palabra = str_remove(palabra, "\\d+")) %>%
  filter(palabra != "") ### NH: Elimina las filas que contienen valores de character vacíos (char vacío no es NA!)

mensajes_limpio

```

Una vez que hemos tokenizado y limpiado nuestro corpus, procederemos a realizar análisis de frecuencias y gráficos.


```{r eval=TRUE}

#Con estas líneas podremos observar palabras y sus frecuencias absolutas

mensajes_palabras %>% 
  count(palabra, sort = TRUE)

mensajes_limpio %>% 
  count(palabra, sort = TRUE)


```

### NH: acá vendría bien algón comentario sobre el resultado anterior, si no, no se entiende para qué se eliminan las stopwords

```{r eval=TRUE}

### NH: en lugar de poner todo esto en la misma celda de código, quedaría mejor que los comentarios sean texto markdown de la notebook seguidos del código en distintas celdas


#Ahora comenzaremos con los análisis del corpus que no fue limpiado 

nrow(mensajes_palabras) / length(archivos)

#En la línea anterior hemos calculado el promedio de palabras por discurso o dnu.

#Con la próxima instrucción calcularemos la frecuencia relativa por palabras y adjudicaremos ese nuevo valor en una columna que se denominará 'relativa'. Por lo cual tendremos tres columnas, la del término, su frecuencia absoluta y la relativa.

mensajes_palabras %>% 
  count (palabra, sort = TRUE) %>% 
  mutate (relativa = n / sum(n))

 # Para poder almacenar estos datos crearemos una nueva tabla denominada 'mensajes_frecuencias'.

mensajes_frecuencias <- mensajes_palabras %>%
           count(palabra, sort = TRUE) %>%
           mutate(relative = n / sum(n))    

#Si quisieramos que nos devuelva la frecuencia por cada discurso o dnu, debemos agrupar por cada uno de ello, lo realizamos con la función 'group_by' y lo guardamos en otra tabla llamada 'frecuencia_text_arch'

frecuencia_text_arch <- mensajes_palabras %>%
           group_by(textos_archivo) %>%
           count(palabra, sort = T) %>%
           mutate(relative = n / sum(n)) %>%
           ungroup()

#Para finalizar podemos graficar todas estas mediciones         
         
mensajes_palabras %>%
           group_by(textos_archivo) %>%
          count(palabra, sort = T) %>%
           ggplot() +
           geom_bar(aes(textos_archivo,
                        n),
                    stat = 'identity')

```



```{r eval=TRUE}
### NH: este texto como markdown en vez de comentario
### NH: el gráfico sería más interesante si ponemos las palabras en eje x y frencuencias en eje y porque se podría observar la ley de Zipf, que la estoy desarrollando en la parte teórica

# Ahora graficaremos las palabras más frecuentes de discursos y dnu, para ello primero llamaremos a nuestra tabla 'mensajes_palabras' luego le diremos que cuente las ocurrencias, ordenadas de mayor a menor, en la tercera línea indicamos que filtre las ocurrencias mayores a 15. Como explicamos antes, como se crea una nueva tabla interna debemos indicar con mutate, la nueva columna, que almacenará los nuevos datos. Luego daremos las indicaciones para graficar con la función 'ggplot'. Primera aclaración, esta función une sus líneas con '+'; el argumento fill hará que cada palabra tenga un color determinado, 'stat' indica que las barras tomaran la altura según 'n'. Las líneas siguientes indicaran título, donde se ubicará la leyenda, y etiquetas en los ejes; y con 'coord_flip()' nos aseguramos que el eje 'Y' se imprima de forma horizontal. 

mensajes_palabras %>%
  count(palabra, sort = T) %>%
  filter(n > 15) %>%
  mutate(palabra = reorder(palabra, n)) %>%
    ggplot(aes(x = palabra, y = n, fill = palabra)) +
    geom_bar(stat="identity") +
    theme_minimal() +
    theme(legend.position = "none") +
    ylab("Número de veces que aparecen") +
    xlab(NULL) +
    ggtitle("Discurso-DNU") +
    coord_flip()
```

         
Ahora, te propongo que realices los análisis y gráficos anteriores para <code>mensajes_limpio</code> y compares los resultados.
