---
title: "ejercicio-analisis-textual"
author: "Romina De León"
date: "21/7/2021"
output: html_document
---

```{css, echo=FALSE}
body {
  padding-bottom: 52px;
}
```

## Comenzando con análisis textual en R Studio

Durante esta ejercitación utilizaras lo visto en las unidades previas. El corpus que he dejado en la carpeta del campus, está compuesto por discursos del presidente A. Fernández y Decretos de Necesidad de Urgencia (DNU) dictados por el Poder Ejecutivo de la Argentina. Estos se presentaron a la población del país desde el inicio del Distanciamiento Social, Preventivo y Obligatorio (DIASPO) el 13 de marzo, hasta el 30 de junio de 2020[^1].

[^1]: Accesibles desde <http://www.infoleg.gob.ar/?page_id=112> y <https://www.casarosada.gob.ar/informacion/discursos?start=200>.

El corpus fue denominado según el siguiente orden: `mes-día-dnu o disc` en formato de texto plano (`txt`). Asimismo, para este ejercicio se utilizarán los primeros cuatro archivos, `0313disc.txt`;`0316dnu.txt`; `0318dnu.txt` y `0320disc.txt`. Esto fue pensado así para poder iniciar pequeñas visualizaciones de esos archivos, donde se podrás observar comparaciones de días, para luego pasar a generalizar con los 47 textos del corpus.

**Primero paso**: deberás armar una carpeta, donde ubiques los archivos a utilizar, en mi ejemplo la denominé **discurso**. Esto es fundamental evitar problemas durante el ejercicio.

```{r eval=FALSE, mensaje=FALSE, warning=FALSE}
getwd()

#Deben modificar su ubicación 

setwd("discurso")

```

El paso a seguir es llamar a las librerías que se utilizaran:

-   <code>tivyverse</code> y <code>tidytext</code> fueron explicados en la [Unidad 1](https://hdlab.space/Estilometria-con-R/unidad1_manipulacion-de-datos.html).
-   <code>ggplot2</code> es un paquete que permite la visualización de datos, se encuentra incluido dentro de **tidyverse**, implementa una representación organizada y en capas, de marcos, ejes, textos, títulos, etc., utilizando diferentes colores, símbolos, tamaños, etc. Fue desarrollada por L. Wilkinson et. al en 2000[^2].

[^2]: Para más información sobre ggplot2, véase Wilkinson, L., D. Wills, D. Rope, A. Norton, and R. Dubbs (2005. The Grammar of Graphics. Statistics and Computing. New York: Springer.

```{r eval=TRUE, message=FALSE, warning=FALSE}

library("tidytext", "tidyverse")

```

Una vez instalados estos requerimientos, llamaras a la función <code>list.file</code> para corroborar que los archivos necesarios se encuentran en la carpeta donde estás trabajando.

```{r eval=FALSE, message=FALSE}

list.files('discurso')
```

Podrás observar que están los archivos que necesitaras.

En un segundo paso, conformaras una nueva **lista**, a la que puedes llamar `archivos`, cuyos elementos serán los textos planos dentro de la carpeta **discurso**, para ello, utilizaras, nuevamente, la función `list.files`, y le indicaras que tipo de archivo debe contener, **txt**, con la declaración `pattern`.

```{r eval=TRUE}

archivos <- list.files(pattern = "\\.txt$")

# Recuerda estar trabajando en la carpeta donde se encuentran tus archivos, sino deberas incorporar un declaración que indique la ubicación, 'path ='.

archivos #corrobora que la lista está correcta

class(archivos) #podrás ver que se trata de una lista de caracteres

length(archivos) #la función 'length' permite obtener el tamaño del objeto que se incluye dentro de los paréntesis

```

Tercer paso, será para facilitar la visualización de cada texto en los siguientes pasos, por lo cual, se eliminará lo que no es necesario del nombre de cada archivo. Para ello, utilizaras la función `gsub`, de gran utilidad para limpieza de datos que está incluida por defecto en **R**. Los parámetros indicaran, según orden de escritura, que se eliminará (\\\\. será para indicar que se busca un **`.`** y no otro carácter según las reglas de expresión regular), que se agregará, las doble comillas vacías, y finalmente, con **perl**, para que se utilice de manera determinada las reglas de expresiones regulares[^3].

[^3]: Puedes acceder a más información de RegEx accediendo a <https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular>.

```{r eval=TRUE, message=TRUE, warning=TRUE}
textos_archivo <- gsub("\\.txt", "", archivos, perl = TRUE)

#Ahora llama a la lista para corroborar que se ha realizado la limpieza

textos_archivo


```

Cuarto paso, con la lista de textos acorde para continuar el ejercicio, procede a armar un `tibble`, este objeto es muy similar a los data.frame presentados en la unidad anterior, pues es rectangular, organizado en filas y columnas, y pertenece al paquete `tidyverse`. Existen tres diferencias entre los data.frame y los tibble, cómo se muestran en consola; los tibble eliminan por defecto **rownames**, e incluso no se recomienda su uso, para evitar problemas de compatibilidad principalmente con bases de datos SQL; por último, no convierte a las **string** (cadenas de caracteres) en factores. Igualmente, ambos objetos son intercambiables[^4].

[^4]: Puedes acceder a más información sobre **tibble** en <https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html>.

Armaras el tibble con tres columnas, la primera y última tendrán caracteres dentro de sus variables, y `parrafo` será numérica, como se muestra a continuación:

```{r eval=TRUE}

mensajes <- tibble(textos_archivo = character(),
                   parrafo = numeric(),
                   texto = character())

#Si llamas a "mensajes" comprobarás que figura como vacío, esto es porque aún no se ha completado con datos, ello lo realizarás en el próximo paso.

mensajes

```

Para poder completar el tibble **mensajes** que has creado, realizarás una iteración con el bucle `for`, que permite repetir instrucciones, evaluando el mismo código para cada elemento de un vector o lista. En este ejercicio, se le indica que comience la iteración en 1 hasta el tamaño (**length**) de `archivos`. Luego, armara en el objeto `discurso` lo que leerá con la función `read_lines`, allí indicaremos que con la función `paste` junte los textos dentro de tu carpeta de trabajo, **importante añadir esa dirección**, con el contenido de `archivos[i]` , cuyo valor cambiará en cada repetición del bucle. El parámetro `sep =` será para indicar que una los con "**/"** . Paso siguiente, convertir lo que acaba de leer en una nueva tabla, otra tibble llamada `temporal`, tal como hiciste la anterior de `mensajes`, indicando que complete con cada texto, la primer columna, `parrafo` indicará el número de párrafos de cada texto, se lo adjudicará con la función `seq_along` que contará hasta el número máximo en el objeto `discurso` y por último, en `texto` se agregará cada párrafo. Para finalizar la repetición, indicaras que en mensajes se sume la tabla `temporal`.

```{r eval=TRUE}

#Iteración 

for (i in 1:length(archivos)){
  discurso <- read_lines(paste("discurso",
                               archivos[i],
                               sep = "/"))
  temporal <- tibble(textos_archivo = textos_archivo[i],
                     parrafo = seq_along(discurso),
                     texto = discurso)
  mensajes <- bind_rows(mensajes, temporal)
} 

mensajes #corrobora que se creó correctamente la tabla
```

Este sexto paso, será para proceder con la tokenización del corpus, es decir extraer sus unidades de análisis. Para ello, se utilizará la función `unnest_tokens` que se encuentra dentro del paquete **tidytext**; se indicará que a cada fila de párrafo, lo separe a nivel de palabra según la columna texto. **mensajes_palabras** estará formado por tres columnas, una que indicará el texto, la otra el número de párrafo y finalmente, la palabra, de cada uno, separada. Recuerda que el operador **pipe (%\>%)** fue presentado en la primera unidad.

```{r eval=TRUE}

mensajes_palabras <- mensajes %>% 
  unnest_tokens(palabra, texto)


mensajes_palabras

```

Como siguiente paso, se eliminaran las palabras vacías, que no serán útil en este ejercicio, pero si pueden servir para otros análisis; asimismo, lo realizarás para continuar con la parte de ejercitación que solicitaré como extra. Para ello, se armara una una tabla con las palabras que se obtienen de la función **get_stopwords** que forma parte del paquete **tidytext**, y se aclarará que se utilizará en español.

```{r eval=TRUE}

stopwords1 <- get_stopwords("es")

stopwords1
```

Paso siguiente, se le indicará que cambie la columna **word** por **palabra**, mediante la función `rename`, que forma parte del paquete **dplyr**, que a su vez pertenece a **tidyverse**, y es útil para cambiar el nombre de variables u objetos. Cuestión que debe considerarse al momento de renombrar, es que primero debe escribirse el nuevo término, y a continuación el que se va a cambiar.

```{r eval=TRUE}

stopwords1 <- stopwords1 %>%
rename(palabra = word)

stopwords1
```

Ahora, para poder obtener una nueva lista con palabras vacías excluidas, se usará la función `anti_join` que compara las columnas con el mismo nombre, en este caso **palabra** y elimina los elementos que coinciden. Cuando finalice la ejecución, aparece un mensaje que significa que las dos listas tiene iguales elementos en esas columnas, en caso contraria, devolverá mensaje de error.

```{r eval=TRUE}

mensajes_sin_vacias <- mensajes_palabras %>%
anti_join(stopwords1)
```

Como paso siguiente, se procederá a la limpieza de la lista **mensajes_palabras** y **mensajes_limpio**. Para ello, se eliminaran los dígitos, las palabras con menos de tres caracteres, cuando la celda se encuentre vacía y si contiene punto, todo ello en la columna **palabra**, puesto que no son útiles para este análisis. Para lo cual, se utilizarán las siguientes funciones:

-   **mutate**, que crea, modifica y elimina columnas según ciertos parámetros que se le adjudican, se encuentra dentro del paquete **tidyverse**. En este caso le indicamos con **str_remove_all** (función que se utiliza para eliminar coincidencias en una cadena de caracteres) que elimine en la columna **palabra** los números que aparezcan.

-   **filter**, será para filtrar las palabras con menos de tres caracteres, mediante **nchar** que cuenta el número de caracteres; también se utilizará para eliminar los espacios con valores de caracteres vacíos y cuando encuentre un **.** o **n°** (símbolo de número que figura en los textos).

Repetiras las limpieza en las dos listas, recuerda guardar las que no fueron limpias, adjudicando nuevos nombres a las que si se hayan limpiado.

```{r echo=TRUE}


mensajes_pal_limpio <- mensajes_palabras %>%
mutate(palabra = str_remove_all(palabra, "\\d+")) %>%
filter(nchar(palabra) > 3) %>% 
filter(palabra != "") %>%   
filter(palabra != ".") %>%   
filter(palabra != "n°")


mensajes_pal_limpio


```

Indica nuevo nombre a esta tabla que se ha limpiado.

```{r echo=TRUE}

mensajes_limpio <- mensajes_sin_vacias %>%
mutate(palabra = str_remove_all(palabra, "\\d+")) %>%
filter(nchar(palabra) > 3) %>% 
filter(palabra != "") %>%   
filter(palabra != ".") %>%   
filter(palabra != "n°")


mensajes_limpio

```

Paso que sigue, se dará inicio a los análisis de frecuencia y gráficos de los textos del corpus. Para lo cual, primero se contará las palabras que se encuentran dentro de la columna **palabra** en **mensajes_pal_limpio**, que lo permitirá la función **count**, también pertenece al paquete **tidyverse**, y presentará la frecuencia absoluta de cada una, o sea la cantidad de veces que se repite.

```{r eval=TRUE}

mensajes_pal_limpio %>% 
  count(palabra, sort = TRUE)

```

Como la frecuencia absoluta no representa ningún dato novedoso, se calculará el promedio de palabras por texto, que no han sido procesadas con la limpieza de datos, pudiéndose calcular entre el resultado del número de columnas obtenidos con la función **nrow** y el tamaño de la lista archivos, calculado en los primeros pasos.

```{r eval=TRUE}

nrow(mensajes_palabras) / length(archivos)


```

Ese dato solo indicará un promedio, por lo cual, a continuación se calculará la frecuencia relativa por palabras y será adjudica como una nueva columna, dentro del tibble limpio: **mensajes_pal_limpio,** que se denominará **relativa**. Por lo cual tendremos tres columnas, la del término, su frecuencia absoluta y la relativa. Por ello armaremos un nuevo tibble denominado, **mensajes_frecuencias**:

```{r eval=TRUE}

mensajes_frec_limpio <- mensajes_pal_limpio %>%
  count (palabra, sort = TRUE) %>%
  mutate (relativa = n / sum(n))

mensajes_frec_limpio
```

También, podrías pedir que devuelva la frecuencia por cada texto, para ello, se deberá agrupar por cada uno mediante la función **group_by**, del paquete **tidyverse**, únicamente indicando que debe considerar la lista de textos, antes preparada; luego que calcule la frecuencia absoluta, que cree una nueva columna para la frecuencia relativa, y finalmente que desagrupe los datos. Todo ello, será guardado en otra tabla **frecuencia_text_arch**:

```{r}


frecuencia_text_arch <- mensajes_pal_limpio %>%
           group_by(textos_archivo) %>%
           count(palabra, sort = T) %>%
           mutate(relative = n / sum(n)) %>%
           ungroup()

frecuencia_text_arch
```

Ahora, para visualizar estos datos calculados realizaras un gráfico, donde se presentará la frecuencia por cada texto, con la función **ggplot**, que fue convocada al principio de la práctica con el paquete **tidyverse**, se le indicará que se graficará en barras, mediante el parámetro **geom_bar**, luego se indicará con **aes**, el contenido de los eje **X** e **Y**, en ese orden; y por último, con **stat**, que tipo de transformación estadísticas tendrán los datos.

```{r eval=TRUE}
  
mensajes_pal_limpio %>%
           group_by(textos_archivo) %>%
          count(palabra, sort = T) %>%
           ggplot() +
           geom_bar(aes(textos_archivo,
                        n),
                    stat = 'identity')


```

Otro gráfico que se puede realizar es observando las palabras más frecuentes de los textos, para ello primero, se llamará a **mensajes_pal_limpio**, se pedirá que cuente las ocurrencias, ordenadas de mayor a menor, en la tercera línea indicamos que filtre las ocurrencias mayores a 10, pues se han eliminado las palabras vacías. Como se explicó antes, al crear una nueva tabla interna se debe indicar con **mutate**, la nueva columna, que almacenará los nuevos datos. Luego se indicaran los parámetros para graficar con **ggplot**. Aclaración, esta función une sus líneas con **+**; el argumento **fill** hará que cada palabra tenga un color determinado, **stat** indicará que las barras tendrán la altura según **n**. Las líneas siguientes presentaran parámetros para título, ubicación de leyenda y etiquetas en los ejes; y con **coord_flip()** se asegura que el eje **Y** se imprima de forma horizontal.

```{r eval=TRUE}

mensajes_pal_limpio %>%
count(palabra, sort = T) %>%
filter(n > 10) %>%
mutate(palabra = reorder(palabra, n)) %>%
ggplot(aes(x = palabra, y = n, fill = palabra)) +
geom_bar(stat="identity") +
theme_minimal() +
theme(legend.position = "none") +
ylab("Número de veces que aparecen") +
xlab(NULL) +
ggtitle("Discurso-DNU") +
coord_flip()


```

Otra utilidad, que ha sido explicada en la parte teórica de la Unidad 2, es la Ley de Zipf, por lo cual, se reorganizaran los parámetros para poder apreciar la relación entre cada palabra y su frecuencia. Para ello, primero se armará una tabla con **mensajes_palabras**, con columnas con frecuencia absoluta = **n** y frecuencia relativa, sumada una última columna, se puede observar en la última línea, que indicará el orden de palabras en forma descendente, o el número de filas, llamada **Clasificacion**.

```{r}

mensajes_frecuencias <- mensajes_palabras %>%
  count (palabra, sort = TRUE) %>%
  mutate (relativa = n / sum(n)) %>% 
   mutate(Clasificacion = row_number())

mensajes_frecuencias
  
```

Para realizar el gráfico con dichos datos, se utilizara la tabla con frecuencia relativa, clasificación y para la varianza de color la frecuencia absoluta. En la tercera línea, con el parámetro **geom_line** se modificará la línea que se grafica; y en las últimas dos, se indica que se debe usar en ambos ejes la escala logarítimica.

```{r eval=TRUE}

mensajes_frecuencias %>%  
  ggplot(aes(Clasificacion, relativa, color = n)) + 
  geom_line(size = 1.5, alpha = 1.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```

Ahora, te propongo que realices los análisis y gráficos anteriores para los mensajes donde se han realizado la limpieza de palabras vacías y compares los resultados. Como ejercitación extra podrías intentar observar los cambios al considerar todos los archivos del corpus.
